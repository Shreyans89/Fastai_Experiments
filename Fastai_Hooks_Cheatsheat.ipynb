{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfa7861-b277-48a3-a3e8-40bdf1032a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c22dc0-8ec0-4edb-9818-48393810679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "#from fastai.tabular.all import *\n",
    "#from fastai.text.all import *\n",
    "#from fastai.collab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de21a1-b195-414a-8cad-0d1f3db7addd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde0943a-6f83-460c-895f-85efbb8ec32a",
   "metadata": {},
   "source": [
    "EXAMPLE OF FWD HOOK: JUST A FUNCTION REGISTERED ON A MODULE (LAYER) OF MODEL   def hook(m,i,o):m->module,i->input to layer,o->output of layer\n",
    "note hoook must be removed ,hence put it in a context manager ,whenever you call model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02f4279-4c37-4229-945b-65d16298c2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True) (tensor([[ 2.0480,  1.1067, -0.4575,  0.1962,  0.5635],\n",
      "        [ 1.5587, -0.0660,  1.6251,  0.4148,  0.7724],\n",
      "        [-1.9013,  0.8115,  2.3466,  1.3560, -0.5092],\n",
      "        [ 0.2373, -1.2778, -0.7176, -0.6380,  1.4384]]),) tensor([[-0.7161,  0.4694,  0.7669],\n",
      "        [-0.9245,  0.8425, -0.3387],\n",
      "        [-0.3217,  0.3720, -1.5483],\n",
      "        [ 0.5831, -0.2265, -0.1864]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "tst_model = nn.Linear(5,3)\n",
    "def example_forward_hook(m,i,o): print(m,i,o)\n",
    "    \n",
    "x = torch.randn(4,5)\n",
    "hook = tst_model.register_forward_hook(example_forward_hook)\n",
    "y = tst_model(x)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78e2d42-37a9-46dd-9187-6a3e0cf93d4b",
   "metadata": {},
   "source": [
    "EXAMPLE OF BCKWDHOOK: JUST A FUNCTION REGISTERED ON A MODULE (LAYER) OF MODEL   def hook(m,gi,go):m->module,gi->GRAD wrt inputof m,go->\n",
    "GRAD wrt output of layer note hoook must be removed ,hence put it in a context manager ,whenever you call model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a813a7-25cb-4cff-8f8c-506048bf3ada",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=3, bias=True) (tensor([0.1896, 0.0178, 0.2865]), None, tensor([[ 0.1093,  0.0411,  0.1934],\n",
      "        [ 0.2579,  0.2280,  0.2913],\n",
      "        [ 0.0554, -0.0198, -0.0082],\n",
      "        [-0.4063, -0.1043, -0.4511],\n",
      "        [ 0.3374,  0.0434,  0.3824]])) (tensor([[ 0.1833,  0.0504,  0.2579],\n",
      "        [ 0.0369,  0.0744,  0.0285],\n",
      "        [ 0.0641, -0.0550,  0.0502],\n",
      "        [-0.0947, -0.0520, -0.0501]]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "def example_backward_hook(m,gi,go): print(m,gi,go)\n",
    "hook = tst_model.register_backward_hook(example_backward_hook)\n",
    "\n",
    "x = torch.randn(4,5)\n",
    "y = tst_model(x)\n",
    "loss = y.pow(2).mean()\n",
    "loss.backward()\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561902f-5c58-4878-8e5a-aa1bac644ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "EXAMPLE OF A HOOK CLASS WHICH IMPLEMENTS THE CONTEXT MANAGER PROTOCOL,so it can be used with the with hook as Hook:\n",
    "                                                                                                       clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2609eb49-32e1-4c83-994b-cb33a6d337ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example_Hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)   \n",
    "\n",
    "    def hook_func(self, m, i, o): self.stored =o.detach().clone()\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfce77d4-17c5-4f9c-824a-2c2df1828820",
   "metadata": {
    "tags": []
   },
   "source": [
    "USING FASTAI's BUILT IN Hook Class... it has context manager functionality built in i.e __enter__ and __exit__ methods defined. Hook(m,hook_fn) \n",
    "takes the folowing params: m (layer/module),hook_fn: def(m,i,o): if forward->hook function to attach to layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3f22bc-2d23-4a9f-8bfa-9473aa23438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = nn.Linear(5,10)\n",
    "x = torch.randn(4,5)\n",
    "y = tst_model(x)\n",
    "with Hook(tst_model, example_forward_hook) as h:\n",
    "    test_stdout(lambda: tst_model(x), f\"{tst_model} ({x},) {y.detach()}\")\n",
    "test_stdout(lambda: tst_model(x), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532ee05-2e33-4374-9c16-40da2f71d0e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this case hook function : lambda m,i,o: o,just stores the output in self.stored=self.hook_fn(m,i,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00ae9a8-54d4-4819-ae89-bf0d11ef56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(5)\n",
    "tst_model = nn.Linear(5,3)\n",
    "hook = Hook(tst_model, lambda m,i,o: o)\n",
    "with torch.no_grad():\n",
    "    y = tst_model(x)\n",
    "test_eq(hook.stored, y)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844408a-2f30-4dc4-afd5-74bc1498840e",
   "metadata": {
    "tags": []
   },
   "source": [
    "USING the \"Hooks FASTAI Builtin class, to store o.p of multiple layers. Also used as a context manager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8f8cd-ab33-42eb-bfef-19ff6f46a740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6415e36e-7874-4b28-b0e2-bbb12b6d4dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4335,  0.0715,  0.0944]), tensor([-0.4335,  0.0715,  0.0944]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425f6d9-a083-4db9-816a-428f717a872a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
